{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a0d5be7-d42b-460b-8d4f-d4af0e96a883",
   "metadata": {},
   "source": [
    "# 神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e6872f-5642-46d7-9bb7-21e677d9b62f",
   "metadata": {},
   "source": [
    "## 1 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf36875-0345-4897-9b43-16129c638f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "import gzip\n",
    "import os\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "\n",
    "def maybe_download(url, filename, expected_bytes, force=False):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    if force or not os.path.exists(filename):\n",
    "        print('Attempting to download:', filename) \n",
    "        # 将远程数据下载到本地\n",
    "        filename, _ = urlretrieve(url + filename, filename)\n",
    "        print('\\nDownload Complete!')\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified', filename)\n",
    "    else:\n",
    "        raise Exception(\n",
    "            'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return filename\n",
    "\n",
    "def read_mnist(fname_img, fname_lbl):\n",
    "    print('\\nReading files %s and %s'%(fname_img, fname_lbl))\n",
    "    \n",
    "    with gzip.open(fname_img) as fimg:        \n",
    "        magic, num, rows, cols = struct.unpack(\">IIII\", fimg.read(16))\n",
    "        print(num,rows,cols)\n",
    "        img = (np.frombuffer(fimg.read(num*rows*cols), dtype=np.uint8).reshape(num, rows * cols)).astype(np.float32)\n",
    "        print('(Images) Returned a tensor of shape ',img.shape)\n",
    "        \n",
    "        img = (img - np.mean(img))/np.std(img)\n",
    "        \n",
    "    with gzip.open(fname_lbl) as flbl:\n",
    "        # flbl.read(8) reads upto 8 bytes\n",
    "        magic, num = struct.unpack(\">II\", flbl.read(8))   \n",
    "        # 将data以流的形式读取，实现动态数组\n",
    "        lbl = np.frombuffer(flbl.read(num), dtype=np.int8)  \n",
    "        print('(Labels) Returned a tensor of shape: %s'%lbl.shape)\n",
    "        print('Sample labels: ',lbl[:10])\n",
    "        \n",
    "    return img, lbl\n",
    "    \n",
    "    \n",
    "# Download data if needed\n",
    "url = 'http://yann.lecun.com/exdb/mnist/'\n",
    "# training data\n",
    "maybe_download(url,'train-images-idx3-ubyte.gz',9912422)  # 训练图片\n",
    "maybe_download(url,'train-labels-idx1-ubyte.gz',28881)  # 训练标签\n",
    "# testing data\n",
    "maybe_download(url,'t10k-images-idx3-ubyte.gz',1648877)  # 预测图片\n",
    "maybe_download(url,'t10k-labels-idx1-ubyte.gz',4542)  # 预测标签\n",
    "\n",
    "# Read the training and testing data \n",
    "train_inputs, train_labels = read_mnist('train-images-idx3-ubyte.gz', 'train-labels-idx1-ubyte.gz')\n",
    "test_inputs, test_labels = read_mnist('t10k-images-idx3-ubyte.gz', 't10k-labels-idx1-ubyte.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943a2c2e-820e-4d8b-b50f-958ad4d2ff97",
   "metadata": {},
   "source": [
    "## 2 定义超参数与常量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28055ba-9f67-4d1a-a1ae-ef7e822b9828",
   "metadata": {},
   "outputs": [],
   "source": [
    "WEIGHTS_STRING = 'weights'\n",
    "BIAS_STRING = 'bias'\n",
    "\n",
    "batch_size = 100  # 一次训练100个样本\n",
    "\n",
    "img_width, img_height = 28,28\n",
    "input_size = img_height * img_width\n",
    "num_labels = 10\n",
    "\n",
    "# resets the default graph Otherwise raises an error about already initialized variables\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a97f78a-2b89-436b-970a-cf7bab270220",
   "metadata": {},
   "source": [
    "## 3 定义输入的占位符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507d9097-37dd-449a-97ea-2c3ea06a75f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_inputs = tf.placeholder(shape=[batch_size, input_size], dtype=tf.float32,\n",
    "                           name='inputs')\n",
    "tf_labels = tf.placeholder(shape=[batch_size, num_labels], dtype=tf.float32,\n",
    "                           name='labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310139d6-e01b-4968-95fb-e5433f0c17d0",
   "metadata": {},
   "source": [
    "**inputs**：对于每一张图, 其像素有$28*28$\n",
    "\n",
    "**labels**：对于每一个标签, 其有0-9共10个值"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e90068-2c70-4742-a8fe-1b94a5c3361a",
   "metadata": {},
   "source": [
    "## 4 定义权重与偏置的参数 (在一个scope里面)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9721474-fc68-4152-917c-2b1d15c897b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_net_parameters():\n",
    "    with tf.variable_scope('layer1'):\n",
    "        tf.get_variable(WEIGHTS_STRING, shape=[input_size, 500], \n",
    "                        initializer=tf.random_normal_initializer(0, 0.02))\n",
    "        tf.get_variable(BIAS_STRING, shape=[500],\n",
    "                        initializer=tf.random_uniform_initializer(0, 0.01))\n",
    "        \n",
    "    with tf.variable_scope('layer2'):\n",
    "        tf.get_variable(WEIGHTS_STRING, shape=[500, 250],\n",
    "                        initializer=tf.random_normal_initializer(0, 0.02))\n",
    "        tf.get_variable(BIAS_STRING, shape=[250], \n",
    "                        initializer=tf.random_uniform_initializer(0, 0.01))\n",
    "        \n",
    "    with tf.variable_scope('output'):\n",
    "        tf.get_variable(WEIGHTS_STRING, shape=[250, 10], \n",
    "                        initializer=tf.random_normal_initializer(0, 0.02))\n",
    "        tf.get_variable(BIAS_STRING, shape=[10], \n",
    "                        initializer=tf.random_uniform_initializer(0, 0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8c2519-de34-4355-99ec-b45a9e8b8d0c",
   "metadata": {},
   "source": [
    "共两层隐层\n",
    "\n",
    "**输入层**: 784个节点\n",
    "\n",
    "**第一层**: 全连接层, $W$为$784 * 500$, 即第一层隐藏层有500个节点; 500个隐层节点对应500个偏置.\n",
    "\n",
    "**第二层**: 全连接层, $W$为$500 * 250$, 即第二层隐藏层有250个节点; 250个隐层节点对应250个偏置.\n",
    "\n",
    "**输出层**: 全连接层, $W$为$250 * 10$, 即输出层有10个节点; 10个输出层节点对应10个偏置."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ed228f-d57e-4fa5-813e-c32dd429aa32",
   "metadata": {},
   "source": [
    "## 5 定义不同作用域中不同参数的作用\n",
    "\n",
    "计算给定$x$的输出 (没有归一化)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df61168c-0af0-44d5-a982-93b3a6de49e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(x):\n",
    "    with tf.variable_scope('layer1', reuse=True):\n",
    "        w, b = tf.get_variable(WEIGHTS_STRING), tf.get_variable(BIAS_STRING)\n",
    "        tf_h1 = tf.nn.relu(tf.matmul(x, w) + b, name='hidden1')\n",
    "        \n",
    "    with tf.variable_scope('layer2', reuse=True):\n",
    "        w, b = tf.get_variable(WEIGHTS_STRING), tf.get_variable(BIAS_STRING)\n",
    "        tf_h2 = tf.nn.relu(tf.matmul(tf_h1, w) + b, name='hidden2')\n",
    "        \n",
    "    with tf.variable_scope('output', reuse=True):\n",
    "        w, b = tf.get_variable(WEIGHTS_STRING), tf.get_variable(BIAS_STRING)\n",
    "        tf_logits = tf.nn.bias_add(tf.matmul(tf_h2, w), b, name='logits')  # 将bias加到矩阵上\n",
    "        \n",
    "    return tf_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561c2e0c-df87-426d-937c-a0ce7917025b",
   "metadata": {},
   "source": [
    "**第一层**: ReLU激活的全连接层\n",
    "\n",
    "**第二层**: ReLU激活的全连接层\n",
    "\n",
    "**输出层**: 全连接层"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524e87b4-c692-4def-a579-f8ede39a008a",
   "metadata": {},
   "source": [
    "## 6 定义损失函数与优化器\n",
    "\n",
    "使用交叉熵作为损失函数\n",
    "\n",
    "使用MomentumOptimizer作为优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639d8369-995c-4e2d-97c1-1a52a30bb74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "define_net_parameters()\n",
    "\n",
    "# defining the loss\n",
    "tf_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=inference(tf_inputs), \n",
    "                                                                    labels=tf_labels))\n",
    "\n",
    "# defining the optimize function\n",
    "tf_loss_minimize = tf.train.MomentumOptimizer(momentum=0.9,learning_rate=0.01).minimize(tf_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b616504-00ae-4e0f-9ee0-666b51ec4040",
   "metadata": {},
   "source": [
    "MomentumOptimizer (动量优化法), 具有加速度的梯度下降方法.\n",
    "\n",
    "使用动量(Momentum)的随机梯度下降法(SGD), 其主要思想是引入一个积攒历史梯度信息动量来加速SGD.\n",
    "\n",
    "其参数更新公式是:\n",
    "$$v_t = \\alpha v_{t-1} + \\eta \\Delta J$$\n",
    "$$W_{t+1}=W_t - v_t$$\n",
    "其中$\\alpha$是超参数, 表示动量的大小, 一般取值0.9; $\\eta$是学习率; $\\Delta J$为损失函数的梯度; $v_t$表示$t$时刻积攒的加速度; $W_t$表示$t$时刻的模型参数.\n",
    "\n",
    "理解策略为：由于当前权值的改变会受到上一次权值改变的影响，类似于小球向下滚动的时候带上了惯性。这样可以加快小球向下滚动的速度。\n",
    "\n",
    "https://blog.csdn.net/weixin_40170902/article/details/80092628"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b159e9d-2589-4756-a5e6-d3cf044ce9bc",
   "metadata": {},
   "source": [
    "## 7 定义预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669eb8e4-d319-4fba-82bf-d2f00f0a4d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_predictions = tf.nn.softmax(inference(tf_inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7ace84-f9dc-4c38-82d3-5691fcdfbe7c",
   "metadata": {},
   "source": [
    "## 8 运行神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f2a70a-8801-410c-b26a-98e8c45b6305",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session = tf.InteractiveSession()\n",
    "\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1807438-271a-4f95-ad07-e15dc0151e93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 50\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "    ''' Measure the classification accuracy of some predictions (softmax outputs) \n",
    "    and labels (integer class labels)'''\n",
    "    return np.sum(np.argmax(predictions,axis=1).flatten()==labels.flatten())/batch_size\n",
    "\n",
    "test_accuracy_over_time = []\n",
    "train_loss_over_time = []\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_loss = []\n",
    "    \n",
    "    # Training Phase \n",
    "    for step in range(train_inputs.shape[0]//batch_size):\n",
    "        # Creating one-hot encoded labels with labels\n",
    "        # One-hot encoding dight 3 for 10-class MNIST data set will result in\n",
    "        # [0,0,0,1,0,0,0,0,0,0]\n",
    "        labels_one_hot = np.zeros((batch_size, num_labels),dtype=np.float32)\n",
    "        labels_one_hot[np.arange(batch_size),train_labels[step*batch_size:(step+1)*batch_size]] = 1.0\n",
    "        \n",
    "        # Printing the one-hot labels\n",
    "        if epoch ==0 and step==0:\n",
    "            print('Sample labels (one-hot)')\n",
    "            print(labels_one_hot[:10])\n",
    "            print()\n",
    "        \n",
    "        # Running the optimization process\n",
    "        loss, _ = session.run([tf_loss,tf_loss_minimize],feed_dict={\n",
    "            tf_inputs: train_inputs[step*batch_size: (step+1)*batch_size,:],\n",
    "            tf_labels: labels_one_hot})\n",
    "        train_loss.append(loss) # Used to average the loss for a single epoch\n",
    "        \n",
    "    test_accuracy = []\n",
    "    # Testing Phase\n",
    "    for step in range(test_inputs.shape[0]//batch_size):\n",
    "        test_predictions = session.run(tf_predictions,feed_dict={tf_inputs: test_inputs[step*batch_size: (step+1)*batch_size,:]})\n",
    "        batch_test_accuracy = accuracy(test_predictions,test_labels[step*batch_size: (step+1)*batch_size])        \n",
    "        test_accuracy.append(batch_test_accuracy)\n",
    "    \n",
    "    print('Average train loss for the %d epoch: %.3f\\n'%(epoch+1,np.mean(train_loss)))\n",
    "    train_loss_over_time.append(np.mean(train_loss))\n",
    "    print('\\tAverage test accuracy for the %d epoch: %.2f\\n'%(epoch+1,np.mean(test_accuracy)*100.0))\n",
    "    test_accuracy_over_time.append(np.mean(test_accuracy)*100)\n",
    "    \n",
    "session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7622c1c4-1f24-4288-91f4-3fb202bfde67",
   "metadata": {},
   "source": [
    "整体运行逻辑是:\n",
    "\n",
    "1. 循环每一个epoch, 每一轮epoch有很多step\n",
    "\n",
    "2. 先将标签进行one-hot编码, 将其从一个标量转换为$1\\times 10$的向量\n",
    "\n",
    "3. 调用损失函数以及损失函数的优化. 调用损失函数的时候, 会将这一step的输入给输入进神经网络, 在神经网络中, 重复使用$W$与$b$\n",
    "\n",
    "4. 在测试数据上测试"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b9d543-20f7-4f9e-9c08-fcc010f0891c",
   "metadata": {},
   "source": [
    "## 9 可视化损失与准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9989ebcd-8031-4ed9-b6e4-2aada622db92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cbe881-daf1-4ed8-ab8f-1a77ca1870bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = np.arange(len(train_loss_over_time))\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "fig.set_size_inches(w=25,h=5)\n",
    "ax[0].plot(x_axis, train_loss_over_time)\n",
    "ax[0].set_xlabel('Epochs',fontsize=18)\n",
    "ax[0].set_ylabel('Average train loss',fontsize=18)\n",
    "ax[0].set_title('Training Loss over Time',fontsize=20)\n",
    "ax[1].plot(x_axis, test_accuracy_over_time)\n",
    "ax[1].set_xlabel('Epochs',fontsize=18)\n",
    "ax[1].set_ylabel('Test accuracy',fontsize=18)\n",
    "ax[1].set_title('Test Accuracy over Time',fontsize=20)\n",
    "fig.savefig('mnist_stats.jpg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf18",
   "language": "python",
   "name": "tf18"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
